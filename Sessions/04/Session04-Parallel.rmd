---
title: "SCRGOT 2023 Coder Upgrade Session 04 - Speeding things up with parallel processing"
author: "Ryan Roberts"
date: "4/15/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(patchwork)
library(rrrSingleCellUtils)
library(Seurat)
library(parallel)

theme_set(theme_classic())
```

## Before we begin, ensure a cluster allocation appropriate for parallel processing

Depending on what environment you are using, you will have either the number of cores that are physically on your local machine (computing locally) or the number of cores assigned to you by the management device (computing remotely on a cluster). For instance, when using slurm on the Franklin cluster, you will need to specify the number of cores requested by using the "-c ##" option when executing the srun or salloc command you used to launch this session. For the purposes of this exercise, we recommend requesting 5-10 cores (or using a local machine that has several cores available). 

You can test to see how many computing cores are available in this session by running the command below:

```{r}
# Run this function to see how many cpu cores you currently have available
parallel::detectCores()

# JUST KIDDING! Be careful to distinguish hardware cores from those
# that have been allotted to you through the scheduler (slurm). One of these
# is actually available to you, the other is not.

# This function respects the scheduler:
parallelly::availableCores()
```

*NOTE: The procedures we will use in this exercise will only work if R is running in a Mac/UNIX/Linux environment. This is because the most efficient (and embarrasingly easy to implement) solutions for multi-core computing in R rely on a process called "forking", which Windows doesn't support. You're OK if you're working on a Windows machine to remote into the cluster, or if you're using a local windows machine to pass commands to a remote Linux machine (like in VSCode). If you need to run multi-core on a Windows machine, check out solutions like the doParallel package.

## Examine the data

The "Datasets" folder contains several single cell datasets downloaded from two different GEO entries. The names of the next level folders (GSEXXXXXX) are the GEO accession IDs. Within these are a series of folders named by the sample ID reported in the publication, each containing the three barcodes/features/matrix files produced by a cellranger alignment. The tibble created below consolidates key data elements associated with each sample.

The samples used to create these datasets were tumors taken from individuals diagnosed with a type of bone tumor (osteosarcoma). Osteosarcoma has several different histologic subtypes (the "path" element below). Primary tumors form in bones, while the lungs are the most common site of metastasis (the "type" argument). There was no additional processing/sorting of the tumor samples before sequencing, so the datasets contain both tumor cells and the surrounding stromal and immune components. The "nCount" column contains a simple pre-determined qc cutoff to reduce doublets.

Within these data are opportunites to address several different scientific questions. Think of some question that you might ask of such data, then select the datasets that you'd use to answer it (for this exercise, ideally 6-10 datasets). Delete or comment out the rows that you won't use, then run the code to create the tibble.

```{r define_data}
geo_data <- tribble(
    ~gse, ~id, ~nCount, ~path, ~type,
    "GSE152048", "BC10", 25000, "Conventional", "Lung Met",
    "GSE152048", "BC11", 30000, "Conventional", "Primary",
    "GSE152048", "BC16", 70000, "Conventional", "Primary",
    "GSE152048", "BC17", 40000, "Chondroblastic", "Lung Met",
    "GSE152048", "BC21", 50000, "Intraosseous", "Primary",
    "GSE152048", "BC22", 50000, "Chondroblastic", "Primary",
    "GSE162454", "OS_1", 50000, "Conventional", "Primary",
    "GSE162454", "OS_2", 45000, "Conventional", "Primary",
    "GSE162454", "OS_3", 23000, "Conventional", "Primary",
    "GSE162454", "OS_4", 50000, "Conventional", "Primary",
    "GSE162454", "OS_5", 50000, "Conventional", "Primary",
    "GSE162454", "OS_6", 45000, "Conventional", "Primary"
)
```

## Practice 1: Simple parallelization with mclapply (from the parallel package)

We will create a list of Seurat objects containing your chosen datasets. We will start by loading and processing the objects 

1. Create a function that will utilize the variables from the tibble you created to convert the data matrices found in the "Datasets" folder into Seurat objects, subsetting for nCount_RNA < $path as a super basic QC. HINT: feel free to use the tenx_load_qc function from rrrSingleCellUtils to streamline the process.
2. Now map the data from the tibble into the function using lapply to create a list of Seurat objects.
3. Then wrap this code with a pair of system timestamps [Sys.time()] and calculate the difference to document the time it takes to perform the operation.
4. Once you have this working, copy that block of code and paste it at the end of the chunk. Then, simply change the "lapply" command to "mclapply" and add the mc.cores argument, with the number of cores set to those you requested in your session.
5. Determine how much time is saved by running the processes in parallel.
6. BONUS. Want to see how all of this is happening by monitoring the processes running on the cluster in real time? Open a separate shell running on the same node and run "top". Then, set the process in motion and observe how your code utilizes the different cores.
7. Now repeat this procedure to create a function that will process your Seurat objects (NormalizeData, FindVariableFeatures, ScaleData, RunPCA, FindNeighbors, FindClusters, RunUMAP), then plot the UMAPs. How much time do you save with this step by running it parallel?
8. BONUS. Restructure your code so that the data loading and processing operations occur in a single parallel computing operation, rather than two separate parallel computing operations. Embed a series of timestamps to benchmark the two approaches and compare the results. Does combining the two operations increase efficiency? Why or why not?
9. BONUS. Are the data objects created using the serial and the parallel processing approaches identical? Why or why not? (see https://pat-s.me/reproducibility-when-going-parallel/ for some helpful information)

```{r parallel-1}
# Create a function to process matrices to a Seurat object
create_seurat <- function(x) {
    x <- tenx_load_qc(
        path_10x = paste0(
            "/home/gdworkshop/lab/session_data/04/Datasets/",
            x$gse, "/",
            x$id),
        violin_plot = FALSE,
        sample_name = x$id
    )
    return(x)
}

# Split the tibble into a list of vectors
geo_data_2 <- geo_data %>%
    as.data.frame() %>%
    split(1:nrow(geo_data)) %>%
    `names<-`(geo_data$id)

# Set the start time
message("Starting serial processing for loading/creating Seurat objects...")
t <- c(Sys.time())

# Map the function onto the list of vectors
tumors_serial <- lapply(geo_data_2, create_seurat)

# Set the completion time for above and start time for below
message("Moving on to parallel processing for loading/creating Seurat objects...")
t[2] <- Sys.time()
print(paste("Serial processing time:",
    difftime(t[2], t[1], units = "secs"),
    "seconds"))

```

## Practice 2: Be a good doobie (time permitting)

***A NOTE ABOUT COMPUTATIONAL STEWARDSHIP***
For the purposes of this course, we are performing parallel computing operations using an interactive session, to which we've assigned several computing resources. Generally, this is a very inefficient way to utilize resources, because you are really only using all of the requested CPUs for brief bursts of activity. The rest of the time, those cores sit idle while you write your code, but are not available for others to use. Leaving idle sessions like this running for long periods of time is poor form on a resource that is free to you (like Franklin) and a good way to spend a lot of money on computing power that you're not actually using. 

While interactive sessions like this can be helpful for development and debugging, they should generally be avoided. Also, when using an interactive session requesting multiple computing nodes, you should try to limit the number of nodes requested to those that you actually need and limit the time that you maintain the allocation (ie, close the session when you are done).

So, is there a way to be more efficient AND be a good citizen of our cyberspace?

YES! Here are some options:
1. Break your code down into sections that can be run as batch submissions through slurm. It sometimes works well to divide your initial processing steps (which are often embarrasingly parallel) from your final processing steps (where you often end up with large/merged objects, performing operations that are difficult to parallelize). You can then run your initial steps with an srun call to rscript with many cores, then bring the resulting processed objects into your interactive session using a handful of cores.
2. Automate submissions to slurm using the rslurm package. Run your interactive session on a single core (only requesting a single core in your srun or salloc interactive session scheduling request).
3. Find a middle ground. Requesting 3 cores will still speed up your data-intense operations about 3-fold, but leaves a lot more computing resources for others. (Or costs a lot less if you're paying for your wall hours.)
4. Make sure you turn things on and off to reduce your footprint. Use srun ... R to start your R sessions whenever you load an interactive session using many cores.

Assignment: Let's go back to our exercise above, but approach it as if you needed to process all 12 samples. Process these samples from load/create Seurat object through to a panel of clustered UMAP plots. Select at least two strategies for speeding up your computational pipeline with a parallel processing strategy. Use benchmarking tools to evaluate the efficiency of your approaches. Be prepared to share the results of your strategy with the rest of the group--we'll learn from what worked well and what didn't. Note that some of these strategies will work well, while others won't--this is part of the exercise.

```{r parallel-slurm-r}
# Let the race begin!
```

## Discussion about parallel computing (especially in R)

This project gives an introduction to a few targeted ways that you can speed up your development process by incorporating parallel computing strategies. This is really just the tip of the iceberg--there is a lot out there (even though R is not the most elegant platform for parallel computing). If you are interested to learn more, check out the following:
  - Incorporation of futures into Seurat to enable parallel-friendly processing: https://satijalab.org/seurat/articles/future_vignette.html (NOTE: You should probably consider this to be an alternative approach to the one we learned above, rather than an opportunity to do both simultaneously, which has some potential for conflicts.)
  - A recent publication that incorporates lazy/streaming data flows (end-to-end C++ single-cell workflow), which is a nice setup to really start utilizing parallel computing infrastructures: https://bnprks.github.io/BPCells/ 
  - The foreach, %dopar% approach, which is quite similar to mclapply used above: https://unc-libraries-data.github.io/R-Open-Labs/Extras/Parallel/foreach.html
  - Making it work on Windows by incorporating parLapply: https://waterprogramming.wordpress.com/2020/03/16/parallel-processing-with-r-on-windows/ 

## Session challenge

To take on today's challenge, use the dataset you generated above and ask a scientific question. This might include some combination of:
  - determine differentially expressed genes between groups or clusters
  - evaluate different parameters for clustering or group assignments
  - identify cell types by comparison to a panel of references
  - isolation/subclustering of particular groups of cells

In order to qualify for judging, you need to incorporate parallel computing into at least three steps in your analysis. Winners will be selected based on the following principles:
 - how appropriate the analyses are for addressing the problem/question
 - how creative the approach is
 - how well the approach is documented
 - how beautifully/accessibly the results are presented
 - how efficiently you leveraged a parallel computing strategy

To submit your code for judging, just save a new file titled "Session04-Challenge-Ryan_Roberts.Rmd" (replacing my name with yours, obviously) in the "Challenges" folder on the SCRGOT Coder Upgrade OneDrive.

```{r}
# Leaving this section for you to explore!
```
